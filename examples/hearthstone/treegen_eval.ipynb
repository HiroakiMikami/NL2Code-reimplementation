{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL2Prog/hearthstone/treegen/evaluate",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yje9hqtcUQ_f",
        "colab_type": "text"
      },
      "source": [
        "### Initialization\n",
        "* Check whether the runtime is host or local.\n",
        "* Mount Google Drive when using the host runtime."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwqGy_GyUQnw",
        "colab_type": "code",
        "outputId": "cafb0d40-2e38-4fce-c667-770c100c4556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/gdrive')\n",
        "    runtime = \"host\"\n",
        "except:\n",
        "    runtime = \"local\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S457sT6QMUr",
        "colab_type": "text"
      },
      "source": [
        "### Parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LYvG4iCQUwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 20367 #@param {type: \"number\"}\n",
        "\n",
        "repository_url = \"https://github.com/HiroakiMikami/NL2Prog\" #@param {type: \"string\"}\n",
        "branch_name = \"master\" #@param {type: \"string\"}\n",
        "\n",
        "word_threshold =  3 #@param {type: \"number\"}\n",
        "token_threshold = 0 #@param {type: \"number\"}\n",
        "max_token_len = 32 #@param {type: \"number\"}\n",
        "max_arity = 16 #@param {type: \"number\"}\n",
        "max_depth = 32 #@param {type: \"number\"}\n",
        "\n",
        "num_heads = 1 #@param {type: \"number\"}\n",
        "num_nl_reader_blocks = 6 #@param {type: \"number\"}\n",
        "num_ast_reader_blocks = 6 #@param {type: \"number\"}\n",
        "num_decoder_blocks = 6 #@param {type: \"number\"}\n",
        "hidden_size = 256 #@param {type: \"number\"}\n",
        "feature_size = 1024  #@param {type: \"number\"}\n",
        "\n",
        "\n",
        "max_action_length = 350 #@param {type: \"number\"}\n",
        "beam_size = 15 #@param {type: \"number\"}\n",
        "\n",
        "dropout = 0.15 #@param {type: \"number\"}\n",
        "\n",
        "device = 0 #@param {type: \"number\"}\n",
        "\n",
        "output_dir_path = \"/gdrive/My Drive/NL2Prog/hearthstone/treegen\" #@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BembldCdOO1",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "* Download the codebase (when using the host runtime)\n",
        "  1. Clone git repository and move to the specified branch\n",
        "  2. Install modules\n",
        "* Use GPU\n",
        "* Fix the random seed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIZJmuz8QFn_",
        "colab_type": "code",
        "outputId": "238f9b7a-a551-43b4-8ea8-80f5dbe9eebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if runtime == \"host\":\n",
        "    %cd /content\n",
        "    !rm -rf NL2Prog\n",
        "    !git clone $repository_url NL2Prog\n",
        "    %cd NL2Prog\n",
        "    !git checkout $branch_name\n",
        "    !pip install .\n",
        "# load tqdm\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsyz7B0Ukxa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "if device != -1:\n",
        "    torch.cuda.set_device(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwjlAkY1fR5j",
        "colab_type": "code",
        "outputId": "6768d7dc-eb42-4274-f52f-ed7248227920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "SEED_MAX = 2**32 - 1\n",
        "\n",
        "root_rng = np.random.RandomState(seed)\n",
        "random.seed(root_rng.randint(SEED_MAX))\n",
        "np.random.seed(root_rng.randint(SEED_MAX))\n",
        "torch.manual_seed(root_rng.randint(SEED_MAX))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz7sdzxUi70b",
        "colab_type": "text"
      },
      "source": [
        "### Setup training\n",
        "* Load the dataset\n",
        "* Split the dataset into train, test, valid\n",
        "* Create and save encoder\n",
        "* Prepare dataset\n",
        "* Create model\n",
        "* Create optimizer\n",
        "* Prepare evaluation\n",
        "* Load checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7kdglcUjDTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nl2prog.dataset.hearthstone import download\n",
        "dataset = download()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrFgwHCntjkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nl2prog.utils.data import ListDataset, Entry\n",
        "test_raw_dataset = dataset[\"test\"]\n",
        "val_raw_dataset = dataset[\"valid\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0WsjTbjHOQ_",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "with open(os.path.join(output_dir_path, \"encoder.pickle\"), \"rb\") as file:\n",
        "    encoder = pickle.load(file)\n",
        "    qencoder = encoder[\"query_encoder\"]\n",
        "    cencoder = encoder[\"character_encoder\"]\n",
        "    aencoder = encoder[\"action_sequence_encoder\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11mMmXAuutb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nl2prog.utils.data import to_eval_dataset\n",
        "\n",
        "\n",
        "test_dataset = to_eval_dataset(test_raw_dataset)\n",
        "valid_dataset = to_eval_dataset(val_raw_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcAaeUZMtqSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nl2prog.nn.treegen import TrainModel\n",
        "model = TrainModel(qencoder, cencoder, aencoder, max_token_len, max_arity,\n",
        "                   max_depth, num_heads, num_nl_reader_blocks,\n",
        "                   num_ast_reader_blocks, num_decoder_blocks, hidden_size,\n",
        "                   feature_size, dropout)\n",
        "if device != -1:\n",
        "    model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViMdu97wimfu",
        "colab_type": "code",
        "outputId": "e52d1f81-d86b-46a8-8817-fb3446bed22a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nl2prog.nn.utils.rnn as nrnn\n",
        "from nl2prog.utils import synthesize as _synthesize, CommonBeamSearchSynthesizer\n",
        "from nl2prog.utils.python import tokenize_query\n",
        "from nl2prog.language.python import is_subtype, parse, unparse\n",
        "from nl2prog.language.action import ActionOptions\n",
        "from nl2prog.metrics import Accuracy\n",
        "from nl2prog.metrics.python import Bleu\n",
        "from nl2prog.utils.data \\\n",
        "    import collate_none, CollateNlFeature, split_none\n",
        "from nl2prog.utils.data.treegen \\\n",
        "    import CollateInput, CollateActionSequence, CollateQuery\n",
        "from nl2prog.utils.transform.treegen import TransformQuery, TransformEvaluator\n",
        "\n",
        "\n",
        "transform_input = TransformQuery(tokenize_query, qencoder, cencoder, max_token_len)\n",
        "transform_evaluator = TransformEvaluator(aencoder, max_arity, max_depth, train=False)\n",
        "synthesizer = CommonBeamSearchSynthesizer(\n",
        "    beam_size, transform_input, transform_evaluator,\n",
        "    CollateInput(torch.device(device)),\n",
        "    CollateActionSequence(torch.device(device)),\n",
        "    CollateQuery(torch.device(device)),\n",
        "    collate_none,\n",
        "    CollateNlFeature(torch.device(device)),\n",
        "    collate_none, split_none,\n",
        "    model.input_reader, model.action_sequence_reader, model.decoder,\n",
        "    model.predictor, aencoder, is_subtype,\n",
        "    options=ActionOptions(True, True),\n",
        "    max_steps=max_action_length)\n",
        "\n",
        "def synthesize(query: str):\n",
        "    return _synthesize(query, synthesizer)\n",
        "\n",
        "accuracy = Accuracy(parse, unparse)\n",
        "bleu = Bleu(parse, unparse)\n",
        "metrics = { \"accuracy\": accuracy, \"bleu\": bleu }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toeP4RGEeK1b",
        "colab_type": "text"
      },
      "source": [
        "### Run Validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAjEaEWYeKGF",
        "colab_type": "code",
        "outputId": "3241ed76-5558-42eb-ab73-dd9253aa1f1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.utils.rnn as rnn\n",
        "import nl2prog.nn.utils.rnn as nrnn\n",
        "from nl2prog.utils import evaluate\n",
        "\n",
        "# Test the model\n",
        "best_score = -1\n",
        "best_score_path = None\n",
        "model_dir_path = os.path.join(output_dir_path, \"models\")\n",
        "for m in os.listdir(model_dir_path):\n",
        "    path = os.path.join(model_dir_path, m)\n",
        "    model.load_state_dict(torch.load(path)[\"model\"])\n",
        "    model.eval()\n",
        "    result = evaluate(tqdm(test_dataset), synthesize, top_n=[1], metrics=metrics)\n",
        "    print(m, result.metrics)\n",
        "    score = result.metrics[1][\"bleu\"]\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_score_path = path\n",
        "print(\"Best Model: {}\".format(best_score_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZ7R3gSK4vU0",
        "outputId": "1d1fbc07-007c-43ba-a1a0-401ebe3317ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn.utils.rnn as rnn\n",
        "import nl2prog.nn.utils.rnn as nrnn\n",
        "from nl2prog.utils import evaluate\n",
        "\n",
        "# Validate the model\n",
        "model.load_state_dict(torch.load(best_score_path)[\"model\"])\n",
        "model.eval()\n",
        "result = evaluate(tqdm(valid_dataset), synthesize, top_n=[1], metrics=metrics)\n",
        "print(result.metrics)\n",
        "with open(os.path.join(output_dir_path, \"validation_results.pickle\"), \"wb\") as file:\n",
        "    pickle.dump(result.results, file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}