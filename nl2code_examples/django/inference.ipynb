{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL2Code django inference",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yje9hqtcUQ_f"
      },
      "source": [
        "### Initialization\n",
        "* Check whether the runtime is host or local.\n",
        "* Mount Google Drive when using the host runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FwqGy_GyUQnw"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "  runtime = \"host\"\n",
        "except:\n",
        "  runtime = \"local\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_S457sT6QMUr"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2LYvG4iCQUwh"
      },
      "outputs": [],
      "source": [
        "#@title Parameters\n",
        "#@markdown |Name            |Description|\n",
        "#@markdown |:---            |:---|\n",
        "#@markdown |`seed`|The random seed|\n",
        "seed = 20367 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ### `nl2code` Repositories\n",
        "#@markdown |Name            |Description|\n",
        "#@markdown |:---            |:---|\n",
        "#@markdown |`repository_url`|The URL of `nl2code` git repository (enabled only in the host runtime)|\n",
        "#@markdown |`branch_name`   |The branch name (enabled only in the host runtime)|\n",
        "repository_url = \"https://github.com/HiroakiMikami/NL2Code-reimplementation\" #@param {type: \"string\"}\n",
        "branch_name = \"master\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown ### Dataset Settings\n",
        "#@markdown |Name               |Description|\n",
        "#@markdown |:---               |:---|\n",
        "#@markdown |`max_action_length`|The maximum action length|\n",
        "max_action_length = 100 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ### Model Parameters\n",
        "#@markdown |Name                     |Description|\n",
        "#@markdown |:---                     |:---|\n",
        "#@markdown |`embedding_dim`          |The dimension of word, token, and rule embeddings|\n",
        "#@markdown |`node_type_embedding_dim`|The dimension of node type embedding dim|\n",
        "#@markdown |`lstm_state_size`        |The size of LSTM state|\n",
        "#@markdwon |`hidden_state_size`      |The size of attention hidden state|\n",
        "embedding_dim = 128 #@param {type: \"number\"}\n",
        "node_type_embedding_dim = 64 #@param {type: \"number\"}\n",
        "lstm_state_size = 256 #@param {type: \"number\"}\n",
        "hidden_state_size = 50 #@param {type: \"number\"}\n",
        "\n",
        "\n",
        "#@markdown ### Inference Settings\n",
        "#@markdown |Name                 |Description|\n",
        "#@markdown |:---                 |:---|\n",
        "#@markdown |`beam_size`          |The beam size|\n",
        "beam_size = 15 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ### Other Settings\n",
        "#@markdown |Name    |Description|\n",
        "#@markdown |:---    |:---|\n",
        "#@markdown |`device`|The id of GPU. `-1` means that CPU is used.|\n",
        "device = 0 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ### Filepathes\n",
        "#@markdown |Name            |Description|\n",
        "#@markdown |:---            |:---|\n",
        "#@markdown |`model_dir_path`|The path of the dataset.|\n",
        "model_dir_path = \"/gdrive/My Drive/NL2Code/django/result\" #@param {type: \"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_BembldCdOO1"
      },
      "source": [
        "### Setup\n",
        "* Download the codebase (when using the host runtime)\n",
        "  1. Clone git repository and move to the specified branch\n",
        "  2. Install modules\n",
        "* Use GPU\n",
        "* Fix the random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FIZJmuz8QFn_"
      },
      "outputs": [],
      "source": [
        "if runtime == \"host\":\n",
        "    %cd /content\n",
        "    !rm -rf NL2Code\n",
        "    ![ ! -e NL2Code ] && git clone $repository_url NL2Code\n",
        "    %cd NL2Code\n",
        "    !git checkout $branch_name\n",
        "    !pip install -e .\n",
        "    !pip install -e . \".[examples]\"\n",
        "# load tqdm\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Rsyz7B0Ukxa7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if device != -1:\n",
        "    torch.cuda.set_device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GwjlAkY1fR5j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "SEED_MAX = 2**32 - 1\n",
        "\n",
        "root_rng = np.random.RandomState(seed)\n",
        "random.seed(root_rng.randint(SEED_MAX))\n",
        "np.random.seed(root_rng.randint(SEED_MAX))\n",
        "torch.manual_seed(root_rng.randint(SEED_MAX))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oz7sdzxUi70b"
      },
      "source": [
        "### Setup inference\n",
        "* Load the encoder\n",
        "* Create and save encoder\n",
        "* Create model\n",
        "* Use GPU (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "V0WsjTbjHOQ_"
      },
      "outputs": [],
      "source": [
        "from nl2code_examples.django import DatasetEncoder\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "with open(os.path.join(model_dir_path, \"encoder.pickle\"), \"rb\") as file:\n",
        "    encoder = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QcAaeUZMtqSt"
      },
      "outputs": [],
      "source": [
        "from nl2code_examples.django import TrainingModel\n",
        "model = TrainingModel(encoder, embedding_dim, node_type_embedding_dim,\n",
        "                      lstm_state_size, hidden_state_size, 0.0)\n",
        "if device != -1:\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJw73TjO1CNc"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "IPJ_QTj3Lfp2"
      },
      "outputs": [],
      "source": [
        "query = \"define the method _create_message with 2 arguments: self and msg\" #@param {type: \"string\"}\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.utils.rnn as rnn\n",
        "from typing import List\n",
        "import nl2code.nn.utils.rnn as nrnn\n",
        "from nl2prog.language.python import is_subtype, to_python_ast\n",
        "from nl2prog.utils.nl2code import BeamSearchSynthesizer\n",
        "from nl2code_examples.django import unparse\n",
        "from nl2code_examples.django._dataset import tokenize_annotation\n",
        "\n",
        "query, query_with_placeholders = tokenize_annotation(query)\n",
        "\n",
        "def query_embedding(query: List[str]):\n",
        "    x = encoder.annotation_encoder.batch_encode(query)\n",
        "    if device != -1:\n",
        "        x = x.cuda()\n",
        "    embedding =  model.encoder(nrnn.pad_sequence([x]))\n",
        "    embedding = embedding.data\n",
        "    return embedding.view(len(query), -1)\n",
        "\n",
        "synthesizer = BeamSearchSynthesizer(beam_size, model.predictor,\n",
        "                                    encoder.action_sequence_encoder, is_subtype,\n",
        "                                    max_steps=max_action_length)\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(model_dir_path, \"best_model.pickle\")))\n",
        "candidates = []\n",
        "for c, _ in synthesizer.synthesize(query, query_embedding(query_with_placeholders)):\n",
        "    candidates.extend(c)\n",
        "candidate = None\n",
        "for c in candidates:\n",
        "    if candidate is None:\n",
        "        candidate = c\n",
        "    else:\n",
        "        if candidate.score < c.score:\n",
        "            candidate = c\n",
        "\n",
        "if candidate is None:\n",
        "    code = \"\"\n",
        "else:\n",
        "    import ast\n",
        "    try:\n",
        "        code = unparse(to_python_ast(candidate.ast))\n",
        "    except:  # noqa\n",
        "        code = \"\"\n",
        "\n",
        "print(\"Top-1\")\n",
        "print(code)\n",
        "\n",
        "print(\"\")\n",
        "print(\"All Candidates\")\n",
        "for c in candidates:\n",
        "    try:\n",
        "        code = unparse(to_python_ast(c.ast))\n",
        "        print(code)\n",
        "        print(\"---\")\n",
        "    except:  # noqa\n",
        "        pass"
      ]
    }
  ]
}