# device
torch.device.type_str = "cuda"
torch.device.index    = 0

# Dataset
dataset/nl2prog.gin.workspace.put.value = @nl2prog.dataset.hearthstone.download()
dataset/nl2prog.gin.workspace.get.key   = "dataset"

# ActionOptions
nl2prog.ast.action.ActionOptions.retain_variadic_fields = True
nl2prog.ast.action.ActionOptions.split_non_terminal     = True

# to_action_sequence
nl2prog.ast.action.code_to_action_sequence.parse    = @nl2prog.language.python.parse
nl2prog.ast.action.code_to_action_sequence.tokenize = @nl2prog.utils.python.tokenize_token
nl2prog.ast.action.code_to_action_sequence.options  = @nl2prog.ast.action.ActionOptions()

# Encoder
train_dataset/nl2prog.gin.get_key.key                  = "train"
train_dataset/nl2prog.gin.get_key.target               = @dataset/nl2prog.gin.workspace.get()
nl2prog.gin.nl2code.prepare_encoder.dataset            = @train_dataset/nl2prog.gin.get_key()
nl2prog.gin.nl2code.prepare_encoder.word_threshold     = 3
nl2prog.gin.nl2code.prepare_encoder.token_threshold    = 0
nl2prog.gin.nl2code.prepare_encoder.parse              = @nl2prog.language.python.parse
nl2prog.gin.nl2code.prepare_encoder.to_action_sequence = @nl2prog.ast.action.code_to_action_sequence
nl2prog.gin.nl2code.prepare_encoder.extract_query      = @nl2prog.utils.python.tokenize_query
nl2prog.gin.nl2code.prepare_encoder.tokenize_token     = @nl2prog.utils.python.tokenize_token
word_encoder/nl2prog.gin.workspace.get.key             = "query_encoder"
action_sequence_encoder/nl2prog.gin.workspace.get.key  = "action_sequence_encoder"

# Transform
nl2prog.utils.transform.TransformDataset.transform_input                   = @nl2prog.utils.transform.nl2code.TransformQuery()
nl2prog.utils.transform.nl2code.TransformQuery.extract_query               = @nl2prog.utils.python.tokenize_query
nl2prog.utils.transform.nl2code.TransformQuery.word_encoder                = @word_encoder/nl2prog.gin.workspace.get()
nl2prog.utils.transform.TransformDataset.transform_code                    = @nl2prog.utils.transform.TransformCode()
nl2prog.utils.transform.TransformCode.to_action_sequence                   = @nl2prog.ast.action.code_to_action_sequence
nl2prog.utils.transform.TransformDataset.transform_evaluator               = @nl2prog.utils.transform.nl2code.TransformEvaluator()
nl2prog.utils.transform.nl2code.TransformEvaluator.action_sequence_encoder = @action_sequence_encoder/nl2prog.gin.workspace.get()
nl2prog.utils.transform.nl2code.TransformEvaluator.train                   = True
nl2prog.utils.transform.TransformDataset.transform_ground_truth            = @nl2prog.utils.transform.TransformGroundTruth()
nl2prog.utils.transform.TransformGroundTruth.action_sequence_encoder       = @action_sequence_encoder/nl2prog.gin.workspace.get()

# Model
nl2prog.nn.nl2code.TrainModel.query_encoder           = @word_encoder/nl2prog.gin.workspace.get()
nl2prog.nn.nl2code.TrainModel.action_sequence_encoder = @action_sequence_encoder/nl2prog.gin.workspace.get()
nl2prog.nn.nl2code.TrainModel.embedding_dim           = 128
nl2prog.nn.nl2code.TrainModel.node_type_embedding_dim = 64
nl2prog.nn.nl2code.TrainModel.lstm_state_size         = 256
nl2prog.nn.nl2code.TrainModel.hidden_state_size       = 50
nl2prog.nn.nl2code.TrainModel.dropout                 = 0.2
model/nl2prog.gin.workspace.put.value                 = @nl2prog.nn.nl2code.TrainModel()
model/nl2prog.gin.workspace.get.key                   = "model"

# Collate
nl2prog.utils.data.Collate.collate_input                = @nl2prog.utils.data.nl2code.CollateInput()
nl2prog.utils.data.Collate.collate_action_sequence      = @nl2prog.utils.data.nl2code.CollateActionSequence()
nl2prog.utils.data.Collate.collate_query                = @nl2prog.utils.data.collate_none
nl2prog.utils.data.Collate.collate_ground_truth         = @nl2prog.utils.data.CollateGroundTruth()
nl2prog.utils.data.nl2code.CollateInput.device          = @torch.device()
nl2prog.utils.data.nl2code.CollateActionSequence.device = @torch.device()
nl2prog.utils.data.CollateGroundTruth.device            = @torch.device()

# Optimizer
nl2prog.gin.optimizer.create_optimizer.optimizer_cls = @torch.optim.Adam
nl2prog.gin.optimizer.create_optimizer.model         = @model/nl2prog.gin.workspace.get()
optimizer/nl2prog.gin.workspace.put.value            = @nl2prog.gin.optimizer.create_optimizer()

# Task
entrypoint.task                              = @nl2prog.gin.nl2prog.train
nl2prog.gin.nl2prog.train.dataset_key        = "dataset"
nl2prog.gin.nl2prog.train.model_key          = "model"
nl2prog.gin.nl2prog.train.optimizer_key      = "optimizer"
nl2prog.gin.nl2prog.train.encoder_keys       = ["query_encoder", "action_sequence_encoder"]
nl2prog.gin.nl2prog.train.workspace_dir      = "output/workspace"
nl2prog.gin.nl2prog.train.output_dir         = "output/output"
nl2prog.gin.nl2prog.train.prepare_dataset    = @dataset/nl2prog.gin.workspace.put
nl2prog.gin.nl2prog.train.prepare_encoder    = @nl2prog.gin.nl2code.prepare_encoder
nl2prog.gin.nl2prog.train.prepare_model      = @model/nl2prog.gin.workspace.put
nl2prog.gin.nl2prog.train.prepare_optimizer  = @optimizer/nl2prog.gin.workspace.put
nl2prog.gin.nl2prog.train.transform_cls      = @nl2prog.utils.transform.TransformDataset
nl2prog.gin.nl2prog.train.loss_fn            = @nl2prog.nn.Loss()
nl2prog.gin.nl2prog.train.score_fn           = @nl2prog.nn.Accuracy()
nl2prog.gin.nl2prog.train.collate_fn         = @nl2prog.utils.data.Collate()
nl2prog.gin.nl2prog.train.batch_size         = 1
nl2prog.gin.nl2prog.train.num_epochs         = 50
nl2prog.gin.nl2prog.train.device             = @torch.device()
