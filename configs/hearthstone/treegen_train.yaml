output_dir: "output/output"

device:
  type: torch.device
  type_str: "cuda"
  index: 0

dataset:
  type: mlprogram.datasets.hearthstone.download
train_dataset: "@/dataset.train"

options:
  type: mlprogram.actions.ActionOptions
  retain_variadic_fields: false
  split_non_terminal: false

to_action_sequence:
  type: mlprogram.utils.Compose
  funcs:
    type: collections.OrderedDict
    items:
      - - "parse"
        - type: mlprogram.languages.python.Parse
      - - "ast_to_action_sequence"
        - type: mlprogram.utils.transform.AstToSingleActionSequence
          tokenize:
            type: mlprogram.languages.python.TokenizeToken
          options: "@/options"

words:
  type: mlprogram.utils.data.get_words
  dataset: "@/train_dataset"
  extract_query:
    type: mlprogram.datasets.hearthstone.TokenizeQuery
chars:
  type: mlprogram.utils.data.get_characters
  dataset: "@/train_dataset"
  extract_query:
    type: mlprogram.datasets.hearthstone.TokenizeQuery
samples:
  type: mlprogram.utils.data.get_samples
  dataset: "@/train_dataset"
  tokenize_token:
    type: mlprogram.languages.python.TokenizeToken
  to_action_sequence: "@/to_action_sequence"

encoder:
  word_encoder:
    type: mlprogram.utils.save
    file:
      type: os.path.join
      args:
        - "@/output_dir"
        - "word_encoder.pt"
    obj:
      type: torchnlp.encoders.LabelEncoder
      sample: "@/words"
      min_occurrences: 3
  char_encoder:
    type: mlprogram.utils.save
    file:
      type: os.path.join
      args:
        - "@/output_dir"
        - "char_encoder.pt"
    obj:
      type: torchnlp.encoders.LabelEncoder
      sample: "@/chars"
      min_occurrences: 0
  action_sequence_encoder:
    type: mlprogram.utils.save
    file:
      type: os.path.join
      args:
        - "@/output_dir"
        - "action_sequence_encoder.pt"
    obj:
      type: mlprogram.encoders.ActionSequenceEncoder
      samples: "@/samples"
      token_threshold: 0

transform:
  type: mlprogram.utils.Sequence
  funcs:
    type: collections.OrderedDict
    items:
      - - "random choice"
        - type: mlprogram.utils.transform.RandomChoice
      - - "transform_input"
        - type: mlprogram.utils.transform.treegen.TransformQuery
          extract_query:
            type: mlprogram.datasets.hearthstone.TokenizeQuery
          word_encoder: "@/encoder.word_encoder"
          char_encoder: "@/encoder.char_encoder"
          max_word_length: 128
      - - "transform_code"
        - type: mlprogram.utils.transform.TransformCode
          to_action_sequence: "@/to_action_sequence"
      - - "transform_action_sequence"
        - type: mlprogram.utils.transform.treegen.TransformActionSequence
          action_sequence_encoder: "@/encoder.action_sequence_encoder"
          max_arity: 128
          max_depth: 128
          train: true
      - - "transform_ground_truth"
        - type: mlprogram.utils.transform.TransformGroundTruth
          action_sequence_encoder: "@/encoder.action_sequence_encoder"

model:
  type: torch.nn.Sequential
  modules:
    type: collections.OrderedDict
    items:
      - - "encoder"
        - type: mlprogram.nn.treegen.NLReader
          token_num: "@/encoder.word_encoder.vocab_size"
          char_num: "@/encoder.char_encoder.vocab_size"
          max_token_len: 128
          char_embed_size: 256
          hidden_size: 256
          n_heads: 1
          dropout: 0.15
          n_blocks: 6
      - - "decoder"
        - type: torch.nn.Sequential
          modules:
            type: collections.OrderedDict
            items:
              - - "action_sequence_reader"
                - type: mlprogram.nn.treegen.ActionSequenceReader
                  rule_num: "@/encoder.action_sequence_encoder._rule_encoder.vocab_size"
                  token_num: "@/encoder.action_sequence_encoder._token_encoder.vocab_size"
                  node_type_num: "@/encoder.action_sequence_encoder._node_type_encoder.vocab_size"
                  max_arity: 128
                  rule_embed_size: 256
                  hidden_size: 256
                  tree_conv_kernel_size: 3
                  n_heads: 1
                  dropout: 0.15
                  n_blocks: 6
              - - "decoder"
                - type: mlprogram.nn.treegen.Decoder
                  rule_num: "@/encoder.action_sequence_encoder._rule_encoder.vocab_size"
                  max_depth: 128
                  feature_size: 256
                  hidden_size: 1024
                  out_size: 256
                  n_heads: 1
                  dropout: 0.15
                  n_blocks: 6
              - - "predictor"
                - type: mlprogram.nn.treegen.Predictor
                  feature_size: 256
                  nl_feature_size: 256
                  rule_size: "@/encoder.action_sequence_encoder._rule_encoder.vocab_size"
                  token_size: "@/encoder.action_sequence_encoder._token_encoder.vocab_size"  # TODO
                  hidden_size: 256

collate:
  type: mlprogram.utils.data.Collate
  device: "@/device"
  word_nl_query:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: true
    dim: 0
    padding_value: -1
  char_nl_query:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: true
    dim: 0
    padding_value: -1
  nl_query_features:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: true
    dim: 0
    padding_value: -1
  actions:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: true
    dim: 0
    padding_value: -1
  previous_actions:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: true
    dim: 0
    padding_value: -1
  previous_action_rules:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: true
    dim: 0
    padding_value: -1
  depthes:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: false
    dim: 1
    padding_value: 0
  adjacency_matrix:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: false
    dim: 0
    padding_value: 0
  action_queries:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: true
    dim: 0
    padding_value: -1
  ground_truth_actions:
    type: mlprogram.utils.data.CollateOptions
    use_pad_sequence: true
    dim: 0
    padding_value: -1

optimizer:
  type: torch.optim.create_optimizer
  optimizer_cls:
    type: fairseq.optim.Adafactor
  model: "@/model"

main:
  type: mlprogram.entrypoint.nl2prog.train
  workspace_dir: "output/workspace"
  output_dir: "@/output_dir"
  dataset:
    type: mlprogram.utils.data.DatasetWithTransform
    dataset: "@/train_dataset"
    transform: "@/transform"
  model: "@/model"
  optimizer: "@/optimizer"
  loss:  
    type: mlprogram.nn.NL2ProgLoss
  score:
    type: mlprogram.nn.NL2ProgAccuracy
  collate: "@/collate"
  batch_size: 1
  num_epochs: 25
  device: "@/device"
