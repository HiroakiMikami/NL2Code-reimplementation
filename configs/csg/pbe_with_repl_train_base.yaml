imports:
    - "pbe_with_repl_base.yaml"

# Overwrite device to use CPU
device:
    type: torch.device
    type_str: "cpu"
    index: 0

output_dir: "output/output"

encoder:
  type: mlprogram.utils.load
  file:
    type: os.path.join
    args:
      - "@/output_dir"
      - "encoder.pt"

optimizer:
    type: torch.optim.Optimizer
    optimizer_cls:
      type: torch.optim.Adam
    model: "@/model"

synthesizer: "@/train_synthesizer"

batch_size: 1
n_rollout: 16

collate_fn:
  type: mlprogram.utils.Sequence
  funcs:
    type: collections.OrderedDict
    items:
      - - "to_episode"
        - type: mlprogram.utils.Map
          func: "@/to_episode"
      - - "flatten"
        - type: mlprogram.utils.Flatten
      - - "transform"
        - type: mlprogram.utils.Map
          func: "@/transform"
      - - "collate"
        - "@/collate"

loss_fn:
  type: torch.nn.Sequential
  modules:
    type: collections.OrderedDict
    items:
      - - "policy"
        - type: torch.nn.Sequential
          modules:
            type: collections.OrderedDict
            items:
              - - "loss"
                - type: mlprogram.nn.action_sequence.Loss
                  reduction: "none"
              - - "weight_by_reward"
                - type: mlprogram.nn.Apply
                  in_keys:
                    - ["reward", "lhs"]
                    - ["action_sequence_loss", "rhs"]
                  out_key: action_sequence_loss
                  module:
                    type: mlprogram.nn.Mul
      - - "value"
        - type: torch.nn.Sequential
          modules:
            type: collections.OrderedDict
            items:
              - - "reshape_reward"
                - type: mlprogram.nn.Apply
                  in_keys:
                    - ["reward", "x"]
                  out_key: value_loss_target
                  module:
                    type: torch.Reshape
                    sizes: [-1, 1]
              - - "BCE"
                - type: mlprogram.nn.Apply
                  in_keys:
                    - ["value", "input"]
                    - ["value_loss_target", "target"]
                  out_key: value_loss
                  module:
                    type: torch.nn.BCELoss
                    reduction: sum
      - - "aggregate"
        - type: mlprogram.nn.Apply
          in_keys:
            - "action_sequence_loss"
            - "value_loss"
          out_key: "loss"
          module:
            type: mlprogram.nn.AggregatedLoss
      - - "normalize"
        - type: mlprogram.nn.Apply
          in_keys:
            - ["loss", "lhs"]
          out_key: "loss"
          module:
            type: mlprogram.nn.Div
          constants:
            "rhs": "@/batch_size"
      - - "pick"
        - type: mlprogram.nn.Pick
          key: loss

main:
  type: mlprogram.entrypoint.train_REINFORCE
  input_dir: "@/output_dir"
  workspace_dir: "output/workspace"
  output_dir: "@/output_dir"
  dataset: "@/train_dataset"
  synthesizer: "@/synthesizer"
  model: "@/model"
  optimizer: "@/optimizer"
  loss: "@/loss_fn"
  evaluate:
    type: mlprogram.entrypoint.EvaluateSynthesizer
    dataset: "@/test_dataset"
    synthesizer: "@/synthesizer"
    metrics: {}
    top_n: []
    n_process: 4
  metric: "generation_rate"
  reward:
    type: mlprogram.metrics.transform
    metric:
      type: mlprogram.metrics.TestCaseResult
      interpreter: "@/interpreter"
      reference: true
      metric:
        type: mlprogram.metrics.Iou
    transform:
      type: mlprogram.utils.Threshold
      threshold: 0.9
      dtype: "float"
  rollout_transform:
    type: mlprogram.utils.transform.RandomChoice
  collate: "@/collate_fn"
  batch_size: "@/batch_size"
  n_rollout: "@/n_rollout"
  length:
    type: mlprogram.entrypoint.train.Iteration
    n: "@/option.n_train_iteration"
  interval:
    type: mlprogram.entrypoint.train.Iteration
    n: 1000
  use_pretrained_model: true
  device: "@/device"
